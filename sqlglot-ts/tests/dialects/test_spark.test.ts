// @generated by codegen_tests.py -- DO NOT EDIT
import { describe, it, expect } from "vitest";
import { transpile } from "../../src/index.js";

const DIALECT = "spark";

function validateIdentity(sql: string, writeSql?: string): void {
  const result = transpile(sql, { readDialect: DIALECT, writeDialect: DIALECT })[0];
  expect(result).toBe(writeSql ?? sql);
}

describe("Spark: ddl", () => {
  it("DAYOFWEEK(TO_DATE(x))", () => {
    validateIdentity("DAYOFWEEK(TO_DATE(x))");
  });
  it("DAYOFMONTH(TO_DATE(x))", () => {
    validateIdentity("DAYOFMONTH(TO_DATE(x))");
  });
  it("DAYOFYEAR(TO_DATE(x))", () => {
    validateIdentity("DAYOFYEAR(TO_DATE(x))");
  });
  it("WEEKOFYEAR(TO_DATE(x))", () => {
    validateIdentity("WEEKOFYEAR(TO_DATE(x))");
  });
  it("SELECT MODE(category)", () => {
    validateIdentity("SELECT MODE(category)");
  });
  it("SELECT MODE(price, TRUE) AS deterministic_mode FROM products", () => {
    validateIdentity("SELECT MODE(price, TRUE) AS deterministic_mode FROM products");
  });
  it.todo("SELECT MODE() WITHIN GROUP (ORDER BY status) FROM orders (unsupported clause)");
  it.todo("DROP NAMESPACE my_catalog.my_namespace (DDL/DML not supported)");
  it.todo("CREATE NAMESPACE my_catalog.my_namespace (DDL/DML not supported)");
  it.todo("INSERT OVERWRITE TABLE db1.tb1 TABLE db2.tb2 (DDL/DML not supported)");
  it.todo("CREATE TABLE foo AS WITH t AS (SELECT 1 AS col) SELECT col FROM t (DDL/DML not supported)");
  it.todo("CREATE TEMPORARY VIEW test AS SELECT 1 (DDL/DML not supported)");
  it.todo("CREATE TABLE foo (col VARCHAR(50)) (DDL/DML not supported)");
  it.todo("CREATE TABLE foo (col STRUCT<struct_col_a: VARCHAR((50))>) (DDL/DML not supported)");
  it.todo("CREATE TABLE foo (col STRING) CLUSTERED BY (col) INTO 10 BUCKETS (DDL/DML not supported)");
  it.todo("CREATE TABLE foo (col STRING) CLUSTERED BY (col) SORTED BY (col) IN... (DDL/DML not supported)");
  it.todo("TRUNCATE TABLE t1 PARTITION(age = 10, name = 'test', address) (DDL/DML not supported)");
  it.todo("CREATE TABLE db.example_table (col_a struct<struct_col_a:int, struc... (DDL/DML not supported)");
  it.todo("CREATE TABLE db.example_table (col_a struct<struct_col_a:int, struc... (DDL/DML not supported) (2)");
  it.todo("CREATE TABLE db.example_table (col_a array<int>, col_b array<array<... (DDL/DML not supported)");
  it.todo("CREATE TABLE x USING ICEBERG PARTITIONED BY (MONTHS(y)) LOCATION 's... (DDL/DML not supported)");
  it.todo("CREATE TABLE test STORED AS PARQUET AS SELECT 1 (DDL/DML not supported)");
  it.todo('CREATE TABLE blah (col_a INT) COMMENT "Test comment: blah" PARTITIO... (pretty=True not supported)');
  it.todo("CACHE TABLE testCache OPTIONS ('storageLevel' 'DISK_ONLY') SELECT *... (command not supported)");
  it.todo("ALTER TABLE StudentInfo ADD COLUMNS (LastName STRING, DOB TIMESTAMP) (DDL/DML not supported)");
  it.todo("ALTER TABLE db.example ALTER COLUMN col_a TYPE BIGINT (DDL/DML not supported)");
  it.todo("ALTER TABLE db.example CHANGE COLUMN col_a col_a BIGINT (DDL/DML not supported)");
  it.todo("ALTER TABLE db.example RENAME COLUMN col_a TO col_b (UnsupportedError in write)");
  it.todo("ALTER TABLE StudentInfo DROP COLUMNS (LastName, DOB) (DDL/DML not supported)");
  it.todo("ALTER VIEW StudentInfoView AS SELECT * FROM StudentInfo (DDL/DML not supported)");
  it.todo("ALTER VIEW StudentInfoView AS SELECT LastName FROM StudentInfo (DDL/DML not supported)");
  it.todo("ALTER VIEW StudentInfoView RENAME TO StudentInfoViewRenamed (DDL/DML not supported)");
  it.todo("ALTER VIEW StudentInfoView SET TBLPROPERTIES ('key1'='val1', 'key2'... (DDL/DML not supported)");
  it.todo("ALTER VIEW StudentInfoView UNSET TBLPROPERTIES ('key1', 'key2') (check_command_warning)");
});

describe("Spark: to_date", () => {
  it.todo("spark -> duckdb: TO_DATE(x, 'yyyy-MM-dd') (cross-dialect transform)");
  it.todo("spark -> hive: TO_DATE(x, 'yyyy-MM-dd') (cross-dialect transform)");
  it.todo("spark -> presto: TO_DATE(x, 'yyyy-MM-dd') (unsupported syntax)");
  it("spark -> spark: TO_DATE(x, 'yyyy-MM-dd')", () => {
    const result = transpile("TO_DATE(x, 'yyyy-MM-dd')", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("TO_DATE(x)");
  });
  it.todo("spark -> snowflake: TO_DATE(x, 'yyyy-MM-dd') (cross-dialect transform)");
  it.todo("spark -> databricks: TO_DATE(x, 'yyyy-MM-dd') (cross-dialect transform)");
  it.todo("spark -> duckdb: TO_DATE(x, 'yyyy') (cross-dialect transform)");
  it.todo("spark -> hive: TO_DATE(x, 'yyyy') (cross-dialect transform)");
  it.todo("spark -> presto: TO_DATE(x, 'yyyy') (cross-dialect transform)");
  it("spark -> spark: TO_DATE(x, 'yyyy')", () => {
    const result = transpile("TO_DATE(x, 'yyyy')", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("TO_DATE(x, 'yyyy')");
  });
  it.todo("spark -> snowflake: TO_DATE(x, 'yyyy') (cross-dialect transform)");
  it.todo("spark -> databricks: TO_DATE(x, 'yyyy') (cross-dialect transform)");
});

describe("Spark: hint", () => {
  it.todo("SELECT /*+ COALESCE(3) */ * FROM x (unsupported syntax)");
  it.todo("SELECT /*+ COALESCE(3), REPARTITION(1) */ * FROM x (unsupported syntax)");
  it.todo("SELECT /*+ BROADCAST(table) */ cola FROM table (unsupported syntax)");
  it.todo("SELECT /*+ BROADCASTJOIN(table) */ cola FROM table (unsupported syntax)");
  it.todo("SELECT /*+ MAPJOIN(table) */ cola FROM table (unsupported syntax)");
  it.todo("SELECT /*+ MERGE(table) */ cola FROM table (unsupported syntax)");
  it.todo("SELECT /*+ SHUFFLEMERGE(table) */ cola FROM table (unsupported syntax)");
  it.todo("SELECT /*+ MERGEJOIN(table) */ cola FROM table (unsupported syntax)");
  it.todo("SELECT /*+ SHUFFLE_HASH(table) */ cola FROM table (unsupported syntax)");
  it.todo("SELECT /*+ SHUFFLE_REPLICATE_NL(table) */ cola FROM table (unsupported syntax)");
});

describe("Spark: spark", () => {
  it.todo("test_spark: assertEqual call");
  it.todo("TRUNC(date_col, 'MM') (assert_is check)");
  it("postgres -> spark: TRUNC(3.14159, 2)", () => {
    const result = transpile("TRUNC(3.14159, 2)", { readDialect: "postgres", writeDialect: DIALECT })[0];
    expect(result).toBe("CAST(3.14159 AS BIGINT)");
  });
  it("SELECT APPROX_TOP_K_ACCUMULATE(col, 10)", () => {
    validateIdentity("SELECT APPROX_TOP_K_ACCUMULATE(col, 10)");
  });
  it("SELECT APPROX_TOP_K_ACCUMULATE(col)", () => {
    validateIdentity("SELECT APPROX_TOP_K_ACCUMULATE(col)");
  });
  it("SELECT BITMAP_BIT_POSITION(10)", () => {
    validateIdentity("SELECT BITMAP_BIT_POSITION(10)");
  });
  it("SELECT BITMAP_CONSTRUCT_AGG(value)", () => {
    validateIdentity("SELECT BITMAP_CONSTRUCT_AGG(value)");
  });
  it.todo("ALTER TABLE foo ADD PARTITION(event = 'click') (DDL/DML not supported)");
  it.todo("ALTER TABLE foo ADD IF NOT EXISTS PARTITION(event = 'click') (DDL/DML not supported)");
  it("IF(cond, foo AS bar, bla AS baz)", () => {
    validateIdentity("IF(cond, foo AS bar, bla AS baz)");
  });
  it("any_value(col, true) -> ANY_VALUE(col) IGNORE NULLS", () => {
    validateIdentity("any_value(col, true)", "ANY_VALUE(col) IGNORE NULLS");
  });
  it("first(col, true) -> FIRST(col) IGNORE NULLS", () => {
    validateIdentity("first(col, true)", "FIRST(col) IGNORE NULLS");
  });
  it("first_value(col, true) -> FIRST_VALUE(col) IGNORE NULLS", () => {
    validateIdentity("first_value(col, true)", "FIRST_VALUE(col) IGNORE NULLS");
  });
  it("last(col, true) -> LAST(col) IGNORE NULLS", () => {
    validateIdentity("last(col, true)", "LAST(col) IGNORE NULLS");
  });
  it("last_value(col, true) -> LAST_VALUE(col) IGNORE NULLS", () => {
    validateIdentity("last_value(col, true)", "LAST_VALUE(col) IGNORE NULLS");
  });
  it.todo("DESCRIBE EXTENDED db.tbl (command not supported)");
  it.todo("SELECT * FROM test TABLESAMPLE (50 PERCENT) (unsupported clause)");
  it.todo("SELECT * FROM test TABLESAMPLE (5 ROWS) (unsupported clause)");
  it.todo("SELECT * FROM test TABLESAMPLE (BUCKET 4 OUT OF 10) (unsupported clause)");
  it.todo("REFRESH 'hdfs://path/to/table' (command not supported)");
  it.todo("REFRESH TABLE tempDB.view1 (command not supported)");
  it("SELECT CASE WHEN a = NULL THEN 1 ELSE 2 END", () => {
    validateIdentity("SELECT CASE WHEN a = NULL THEN 1 ELSE 2 END");
  });
  it("SELECT * FROM t1 SEMI JOIN t2 ON t1.x = t2.x", () => {
    validateIdentity("SELECT * FROM t1 SEMI JOIN t2 ON t1.x = t2.x");
  });
  it.todo("SELECT TRANSFORM(ARRAY(1, 2, 3), x -> x + 1) (unsupported syntax)");
  it.todo("SELECT TRANSFORM(ARRAY(1, 2, 3), (x, i) -> x + i) (unsupported syntax)");
  it.todo("REFRESH TABLE a.b.c (command not supported)");
  it.todo("INTERVAL '-86' DAYS (unsupported syntax)");
  it("TRIM('    SparkSQL   ')", () => {
    validateIdentity("TRIM('    SparkSQL   ')");
  });
  it.todo("TRIM(BOTH 'SL' FROM 'SSparkSQLS') (unsupported syntax)");
  it.todo("TRIM(LEADING 'SL' FROM 'SSparkSQLS') (unsupported syntax)");
  it.todo("TRIM(TRAILING 'SL' FROM 'SSparkSQLS') (unsupported syntax)");
  it("SPLIT(str, pattern, lim)", () => {
    validateIdentity("SPLIT(str, pattern, lim)");
  });
  it("SELECT * FROM t1, t2 -> SELECT * FROM t1 CROSS JOIN t2", () => {
    validateIdentity("SELECT * FROM t1, t2", "SELECT * FROM t1 CROSS JOIN t2");
  });
  it("SELECT 1 limit -> SELECT 1 AS limit", () => {
    validateIdentity("SELECT 1 limit", "SELECT 1 AS limit");
  });
  it("SELECT 1 offset -> SELECT 1 AS offset", () => {
    validateIdentity("SELECT 1 offset", "SELECT 1 AS offset");
  });
  it("SELECT UNIX_TIMESTAMP() -> SELECT UNIX_TIMESTAMP(CURRENT_TIMESTAMP())", () => {
    validateIdentity("SELECT UNIX_TIMESTAMP()", "SELECT UNIX_TIMESTAMP(CURRENT_TIMESTAMP())");
  });
  it.todo("SELECT CAST('2023-01-01' AS TIMESTAMP) + INTERVAL 23 HOUR + 59 MINU... (unsupported syntax)");
  it.todo("SELECT CAST('2023-01-01' AS TIMESTAMP) + INTERVAL '23' HOUR + '59' ... (unsupported syntax)");
  it.todo("SELECT INTERVAL '5' HOURS '30' MINUTES '5' SECONDS '6' MILLISECONDS... (unsupported syntax)");
  it("SELECT INTERVAL 5 HOURS 30 MINUTES 5 SECONDS 6 MILLISECONDS 7 MICROSECONDS -> SELECT IN...", () => {
    validateIdentity("SELECT INTERVAL 5 HOURS 30 MINUTES 5 SECONDS 6 MILLISECONDS 7 MICROSECONDS", "SELECT INTERVAL '5' HOURS + INTERVAL '30' MINUTES + INTERVAL '5' SECONDS + INTERVAL '6' MILLISECONDS + INTERVAL '7' MICROSECONDS");
  });
  it("SELECT REGEXP_REPLACE('100-200', r'([^0-9])', '') -> SELECT REGEXP_REPLACE('100-200', '...", () => {
    validateIdentity("SELECT REGEXP_REPLACE('100-200', r'([^0-9])', '')", "SELECT REGEXP_REPLACE('100-200', '([^0-9])', '')");
  });
  it("SELECT REGEXP_REPLACE('100-200', R'([^0-9])', '') -> SELECT REGEXP_REPLACE('100-200', '...", () => {
    validateIdentity("SELECT REGEXP_REPLACE('100-200', R'([^0-9])', '')", "SELECT REGEXP_REPLACE('100-200', '([^0-9])', '')");
  });
  it("SELECT STR_TO_MAP('a:1,b:2,c:3') -> SELECT STR_TO_MAP('a:1,b:2,c:3', ',', ':')", () => {
    validateIdentity("SELECT STR_TO_MAP('a:1,b:2,c:3')", "SELECT STR_TO_MAP('a:1,b:2,c:3', ',', ':')");
  });
  it("duckdb -> spark: SELECT * FROM READ_PARQUET('name.parquet')", () => {
    const result = transpile("SELECT * FROM READ_PARQUET('name.parquet')", { readDialect: "duckdb", writeDialect: DIALECT })[0];
    expect(result).toBe("SELECT * FROM parquet.`name.parquet`");
  });
  it("spark -> spark: SELECT * FROM parquet.`name.parquet`", () => {
    const result = transpile("SELECT * FROM parquet.`name.parquet`", { readDialect: "spark", writeDialect: DIALECT })[0];
    expect(result).toBe("SELECT * FROM parquet.`name.parquet`");
  });
  it.todo("spark -> presto: SELECT TO_JSON(STRUCT('blah' AS x)) AS y (unsupported syntax)");
  it("spark -> spark: SELECT TO_JSON(STRUCT('blah' AS x)) AS y", () => {
    const result = transpile("SELECT TO_JSON(STRUCT('blah' AS x)) AS y", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT TO_JSON(STRUCT('blah' AS x)) AS y");
  });
  it.todo("spark -> trino: SELECT TO_JSON(STRUCT('blah' AS x)) AS y (unsupported syntax)");
  it.todo("SELECT TRY_ELEMENT_AT(ARRAY(1, 2, 3), 2) (unsupported syntax)");
  it.todo("SELECT id_column, name, age FROM test_table LATERAL VIEW INLINE(str... (unsupported clause)");
  it.todo("SELECT ARRAY_AGG(x) FILTER (WHERE x = 5) FROM (SELECT 1 UNION ALL S... (unsupported syntax)");
  it("spark -> duckdb: SELECT ARRAY_AGG(1)", () => {
    const result = transpile("SELECT ARRAY_AGG(1)", { readDialect: DIALECT, writeDialect: "duckdb" })[0];
    expect(result).toBe("SELECT ARRAY_AGG(1)");
  });
  it("spark -> spark: SELECT ARRAY_AGG(1)", () => {
    const result = transpile("SELECT ARRAY_AGG(1)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT COLLECT_LIST(1)");
  });
  it("spark -> duckdb: SELECT ARRAY_AGG(DISTINCT STRUCT('a'))", () => {
    const result = transpile("SELECT ARRAY_AGG(DISTINCT STRUCT('a'))", { readDialect: DIALECT, writeDialect: "duckdb" })[0];
    expect(result).toBe("SELECT ARRAY_AGG(DISTINCT {'col1': 'a'})");
  });
  it("spark -> spark: SELECT ARRAY_AGG(DISTINCT STRUCT('a'))", () => {
    const result = transpile("SELECT ARRAY_AGG(DISTINCT STRUCT('a'))", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT COLLECT_LIST(DISTINCT STRUCT('a' AS col1))");
  });
  it.todo("SELECT DATE_FORMAT(DATE '2020-01-01', 'EEEE') AS weekday (unsupported syntax)");
  it("databricks -> spark: SELECT TRY_ELEMENT_AT(MAP(1, 'a', 2, 'b'), 2)", () => {
    const result = transpile("SELECT TRY_ELEMENT_AT(MAP(1, 'a', 2, 'b'), 2)", { readDialect: "databricks", writeDialect: DIALECT })[0];
    expect(result).toBe("SELECT TRY_ELEMENT_AT(MAP(1, 'a', 2, 'b'), 2)");
  });
  it("spark -> databricks: SELECT TRY_ELEMENT_AT(MAP(1, 'a', 2, 'b'), 2)", () => {
    const result = transpile("SELECT TRY_ELEMENT_AT(MAP(1, 'a', 2, 'b'), 2)", { readDialect: DIALECT, writeDialect: "databricks" })[0];
    expect(result).toBe("SELECT TRY_ELEMENT_AT(MAP(1, 'a', 2, 'b'), 2)");
  });
  it("spark -> duckdb: SELECT TRY_ELEMENT_AT(MAP(1, 'a', 2, 'b'), 2)", () => {
    const result = transpile("SELECT TRY_ELEMENT_AT(MAP(1, 'a', 2, 'b'), 2)", { readDialect: DIALECT, writeDialect: "duckdb" })[0];
    expect(result).toBe("SELECT MAP([1, 2], ['a', 'b'])[2]");
  });
  it("spark -> duckdb, version=1.1.0: SELECT TRY_ELEMENT_AT(MAP(1, 'a', 2, 'b'), 2)", () => {
    const result = transpile("SELECT TRY_ELEMENT_AT(MAP(1, 'a', 2, 'b'), 2)", { readDialect: DIALECT, writeDialect: "duckdb, version=1.1.0" })[0];
    expect(result).toBe("SELECT (MAP([1, 2], ['a', 'b'])[2])[1]");
  });
  it("spark -> spark: SELECT TRY_ELEMENT_AT(MAP(1, 'a', 2, 'b'), 2)", () => {
    const result = transpile("SELECT TRY_ELEMENT_AT(MAP(1, 'a', 2, 'b'), 2)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT TRY_ELEMENT_AT(MAP(1, 'a', 2, 'b'), 2)");
  });
  it("duckdb -> spark: SELECT STR_SPLIT_REGEX('123|789', '\\|')", () => {
    const result = transpile("SELECT STR_SPLIT_REGEX('123|789', '\\|')", { readDialect: "duckdb", writeDialect: DIALECT })[0];
    expect(result).toBe("SELECT SPLIT('123|789', '\\\\|')");
  });
  it("presto -> spark: SELECT REGEXP_SPLIT('123|789', '\\|')", () => {
    const result = transpile("SELECT REGEXP_SPLIT('123|789', '\\|')", { readDialect: "presto", writeDialect: DIALECT })[0];
    expect(result).toBe("SELECT SPLIT('123|789', '\\\\|')");
  });
  it("spark -> duckdb: SELECT SPLIT('123|789', '\\\\|')", () => {
    const result = transpile("SELECT SPLIT('123|789', '\\\\|')", { readDialect: DIALECT, writeDialect: "duckdb" })[0];
    expect(result).toBe("SELECT STR_SPLIT_REGEX('123|789', '\\|')");
  });
  it("spark -> presto: SELECT SPLIT('123|789', '\\\\|')", () => {
    const result = transpile("SELECT SPLIT('123|789', '\\\\|')", { readDialect: DIALECT, writeDialect: "presto" })[0];
    expect(result).toBe("SELECT REGEXP_SPLIT('123|789', '\\|')");
  });
  it("spark -> spark: SELECT SPLIT('123|789', '\\\\|')", () => {
    const result = transpile("SELECT SPLIT('123|789', '\\\\|')", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT SPLIT('123|789', '\\\\|')");
  });
  it("spark -> clickhouse: WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL ...", () => {
    const result = transpile("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT id, name) AS cnt FROM tbl", { readDialect: DIALECT, writeDialect: "clickhouse" })[0];
    expect(result).toBe("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT id, name) AS cnt FROM tbl");
  });
  it("spark -> databricks: WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL ...", () => {
    const result = transpile("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT id, name) AS cnt FROM tbl", { readDialect: DIALECT, writeDialect: "databricks" })[0];
    expect(result).toBe("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT id, name) AS cnt FROM tbl");
  });
  it("spark -> doris: WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id...", () => {
    const result = transpile("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT id, name) AS cnt FROM tbl", { readDialect: DIALECT, writeDialect: "doris" })[0];
    expect(result).toBe("WITH tbl AS (SELECT 1 AS id, 'eggy' AS `name` UNION ALL SELECT NULL AS id, 'jake' AS `name`) SELECT COUNT(DISTINCT id, `name`) AS cnt FROM tbl");
  });
  it("spark -> duckdb: WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS i...", () => {
    const result = transpile("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT id, name) AS cnt FROM tbl", { readDialect: DIALECT, writeDialect: "duckdb" })[0];
    expect(result).toBe("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT CASE WHEN id IS NULL THEN NULL WHEN name IS NULL THEN NULL ELSE (id, name) END) AS cnt FROM tbl");
  });
  it("spark -> hive: WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id,...", () => {
    const result = transpile("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT id, name) AS cnt FROM tbl", { readDialect: DIALECT, writeDialect: "hive" })[0];
    expect(result).toBe("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT id, name) AS cnt FROM tbl");
  });
  it("spark -> mysql: WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id...", () => {
    const result = transpile("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT id, name) AS cnt FROM tbl", { readDialect: DIALECT, writeDialect: "mysql" })[0];
    expect(result).toBe("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT id, name) AS cnt FROM tbl");
  });
  it("spark -> postgres: WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS...", () => {
    const result = transpile("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT id, name) AS cnt FROM tbl", { readDialect: DIALECT, writeDialect: "postgres" })[0];
    expect(result).toBe("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT CASE WHEN id IS NULL THEN NULL WHEN name IS NULL THEN NULL ELSE (id, name) END) AS cnt FROM tbl");
  });
  it("spark -> presto: WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS i...", () => {
    const result = transpile("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT id, name) AS cnt FROM tbl", { readDialect: DIALECT, writeDialect: "presto" })[0];
    expect(result).toBe("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT CASE WHEN id IS NULL THEN NULL WHEN name IS NULL THEN NULL ELSE (id, name) END) AS cnt FROM tbl");
  });
  it("spark -> snowflake: WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL A...", () => {
    const result = transpile("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT id, name) AS cnt FROM tbl", { readDialect: DIALECT, writeDialect: "snowflake" })[0];
    expect(result).toBe("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT id, name) AS cnt FROM tbl");
  });
  it("spark -> spark: WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id...", () => {
    const result = transpile("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT id, name) AS cnt FROM tbl", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("WITH tbl AS (SELECT 1 AS id, 'eggy' AS name UNION ALL SELECT NULL AS id, 'jake' AS name) SELECT COUNT(DISTINCT id, name) AS cnt FROM tbl");
  });
  it("spark -> bigquery: SELECT TO_UTC_TIMESTAMP('2016-08-31', 'Asia/Seoul')", () => {
    const result = transpile("SELECT TO_UTC_TIMESTAMP('2016-08-31', 'Asia/Seoul')", { readDialect: DIALECT, writeDialect: "bigquery" })[0];
    expect(result).toBe("SELECT DATETIME(TIMESTAMP(CAST('2016-08-31' AS DATETIME), 'Asia/Seoul'), 'UTC')");
  });
  it.todo("spark -> duckdb: SELECT TO_UTC_TIMESTAMP('2016-08-31', 'Asia/Seoul') (unsupported syntax)");
  it.todo("spark -> postgres: SELECT TO_UTC_TIMESTAMP('2016-08-31', 'Asia/Seoul') (unsupported syntax)");
  it.todo("spark -> presto: SELECT TO_UTC_TIMESTAMP('2016-08-31', 'Asia/Seoul') (unsupported syntax)");
  it.todo("spark -> redshift: SELECT TO_UTC_TIMESTAMP('2016-08-31', 'Asia/Seoul') (unsupported syntax)");
  it.todo("spark -> snowflake: SELECT TO_UTC_TIMESTAMP('2016-08-31', 'Asia/Seoul') (unsupported syntax)");
  it.todo("spark -> spark: SELECT TO_UTC_TIMESTAMP('2016-08-31', 'Asia/Seoul') (unsupported syntax)");
  it.todo("spark -> presto: SELECT FROM_UTC_TIMESTAMP('2016-08-31', 'Asia/Seoul') (unsupported syntax)");
  it.todo("spark -> spark: SELECT FROM_UTC_TIMESTAMP('2016-08-31', 'Asia/Seoul') (unsupported syntax)");
  it(" -> spark: STRUCT_EXTRACT(foo, bar)", () => {
    const result = transpile("STRUCT_EXTRACT(foo, bar)", { readDialect: "", writeDialect: DIALECT })[0];
    expect(result).toBe("foo.bar");
  });
  it("spark -> spark: MAP(1, 2, 3, 4)", () => {
    const result = transpile("MAP(1, 2, 3, 4)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("MAP(1, 2, 3, 4)");
  });
  it.todo("spark -> trino: MAP(1, 2, 3, 4) (unsupported syntax)");
  it("spark -> spark: MAP()", () => {
    const result = transpile("MAP()", { readDialect: "spark", writeDialect: DIALECT })[0];
    expect(result).toBe("MAP()");
  });
  it("trino -> spark: MAP()", () => {
    const result = transpile("MAP()", { readDialect: "trino", writeDialect: DIALECT })[0];
    expect(result).toBe("MAP()");
  });
  it.todo("spark -> trino: MAP() (unsupported syntax)");
  it("presto -> spark: SELECT SPLIT_TO_MAP('a:1,b:2,c:3', ',', ':')", () => {
    const result = transpile("SELECT SPLIT_TO_MAP('a:1,b:2,c:3', ',', ':')", { readDialect: "presto", writeDialect: DIALECT })[0];
    expect(result).toBe("SELECT STR_TO_MAP('a:1,b:2,c:3', ',', ':')");
  });
  it("spark -> spark: SELECT STR_TO_MAP('a:1,b:2,c:3', ',', ':')", () => {
    const result = transpile("SELECT STR_TO_MAP('a:1,b:2,c:3', ',', ':')", { readDialect: "spark", writeDialect: DIALECT })[0];
    expect(result).toBe("SELECT STR_TO_MAP('a:1,b:2,c:3', ',', ':')");
  });
  it("spark -> presto: SELECT STR_TO_MAP('a:1,b:2,c:3', ',', ':')", () => {
    const result = transpile("SELECT STR_TO_MAP('a:1,b:2,c:3', ',', ':')", { readDialect: DIALECT, writeDialect: "presto" })[0];
    expect(result).toBe("SELECT SPLIT_TO_MAP('a:1,b:2,c:3', ',', ':')");
  });
  it("spark -> spark: SELECT STR_TO_MAP('a:1,b:2,c:3', ',', ':') (2)", () => {
    const result = transpile("SELECT STR_TO_MAP('a:1,b:2,c:3', ',', ':')", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT STR_TO_MAP('a:1,b:2,c:3', ',', ':')");
  });
  it.todo("SELECT DATEDIFF(MONTH, CAST('1996-10-30' AS TIMESTAMP), CAST('1997-... (unsupported syntax)");
  it.todo("spark -> bigquery: SELECT DATEDIFF(week, '2020-01-01', '2020-12-31') (cross-dialect transform)");
  it.todo("spark -> duckdb: SELECT DATEDIFF(week, '2020-01-01', '2020-12-31') (cross-dialect transform)");
  it.todo("spark -> hive: SELECT DATEDIFF(week, '2020-01-01', '2020-12-31') (cross-dialect transform)");
  it.todo("spark -> postgres: SELECT DATEDIFF(week, '2020-01-01', '2020-12-31') (cross-dialect transform)");
  it.todo("spark -> redshift: SELECT DATEDIFF(week, '2020-01-01', '2020-12-31') (cross-dialect transform)");
  it.todo("spark -> snowflake: SELECT DATEDIFF(week, '2020-01-01', '2020-12-31') (cross-dialect transform)");
  it("spark -> spark: SELECT DATEDIFF(week, '2020-01-01', '2020-12-31')", () => {
    const result = transpile("SELECT DATEDIFF(week, '2020-01-01', '2020-12-31')", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT DATEDIFF(WEEK, '2020-01-01', '2020-12-31')");
  });
  it.todo("spark -> duckdb: SELECT MONTHS_BETWEEN('1997-02-28 10:30:00', '1996-10-30') (cross-dialect transform)");
  it("spark -> hive: SELECT MONTHS_BETWEEN('1997-02-28 10:30:00', '1996-10-30')", () => {
    const result = transpile("SELECT MONTHS_BETWEEN('1997-02-28 10:30:00', '1996-10-30')", { readDialect: DIALECT, writeDialect: "hive" })[0];
    expect(result).toBe("SELECT MONTHS_BETWEEN('1997-02-28 10:30:00', '1996-10-30')");
  });
  it("spark -> spark: SELECT MONTHS_BETWEEN('1997-02-28 10:30:00', '1996-10-30')", () => {
    const result = transpile("SELECT MONTHS_BETWEEN('1997-02-28 10:30:00', '1996-10-30')", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT MONTHS_BETWEEN('1997-02-28 10:30:00', '1996-10-30')");
  });
  it.todo("spark -> duckdb: SELECT MONTHS_BETWEEN('1997-02-28 10:30:00', '1996-10-30', FALSE) (cross-dialect transform)");
  it("spark -> hive: SELECT MONTHS_BETWEEN('1997-02-28 10:30:00', '1996-10-30', FALSE)", () => {
    const result = transpile("SELECT MONTHS_BETWEEN('1997-02-28 10:30:00', '1996-10-30', FALSE)", { readDialect: DIALECT, writeDialect: "hive" })[0];
    expect(result).toBe("SELECT MONTHS_BETWEEN('1997-02-28 10:30:00', '1996-10-30')");
  });
  it("spark -> spark: SELECT MONTHS_BETWEEN('1997-02-28 10:30:00', '1996-10-30', FALSE)", () => {
    const result = transpile("SELECT MONTHS_BETWEEN('1997-02-28 10:30:00', '1996-10-30', FALSE)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT MONTHS_BETWEEN('1997-02-28 10:30:00', '1996-10-30', FALSE)");
  });
  it.todo("spark -> : SELECT TO_TIMESTAMP('2016-12-31 00:12:00') (unsupported syntax)");
  it.todo("spark -> duckdb: SELECT TO_TIMESTAMP('2016-12-31 00:12:00') (unsupported syntax)");
  it.todo("spark -> spark: SELECT TO_TIMESTAMP('2016-12-31 00:12:00') (unsupported syntax)");
  it.todo("spark -> : SELECT TO_TIMESTAMP(x, 'zZ') (cross-dialect transform)");
  it.todo("spark -> duckdb: SELECT TO_TIMESTAMP(x, 'zZ') (cross-dialect transform)");
  it("duckdb -> spark: SELECT STRPTIME('2016-12-31', '%Y-%m-%d')", () => {
    const result = transpile("SELECT STRPTIME('2016-12-31', '%Y-%m-%d')", { readDialect: "duckdb", writeDialect: DIALECT })[0];
    expect(result).toBe("SELECT TO_TIMESTAMP('2016-12-31', 'yyyy-MM-dd')");
  });
  it.todo("spark -> : SELECT TO_TIMESTAMP('2016-12-31', 'yyyy-MM-dd') (cross-dialect transform)");
  it.todo("spark -> duckdb: SELECT TO_TIMESTAMP('2016-12-31', 'yyyy-MM-dd') (cross-dialect transform)");
  it("spark -> spark: SELECT TO_TIMESTAMP('2016-12-31', 'yyyy-MM-dd')", () => {
    const result = transpile("SELECT TO_TIMESTAMP('2016-12-31', 'yyyy-MM-dd')", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT TO_TIMESTAMP('2016-12-31', 'yyyy-MM-dd')");
  });
  it("spark -> bigquery: SELECT RLIKE('John Doe', 'John.*')", () => {
    const result = transpile("SELECT RLIKE('John Doe', 'John.*')", { readDialect: DIALECT, writeDialect: "bigquery" })[0];
    expect(result).toBe("SELECT REGEXP_CONTAINS('John Doe', 'John.*')");
  });
  it("spark -> hive: SELECT RLIKE('John Doe', 'John.*')", () => {
    const result = transpile("SELECT RLIKE('John Doe', 'John.*')", { readDialect: DIALECT, writeDialect: "hive" })[0];
    expect(result).toBe("SELECT 'John Doe' RLIKE 'John.*'");
  });
  it.todo("spark -> postgres: SELECT RLIKE('John Doe', 'John.*') (unsupported syntax)");
  it("spark -> snowflake: SELECT RLIKE('John Doe', 'John.*')", () => {
    const result = transpile("SELECT RLIKE('John Doe', 'John.*')", { readDialect: DIALECT, writeDialect: "snowflake" })[0];
    expect(result).toBe("SELECT REGEXP_LIKE('John Doe', 'John.*')");
  });
  it("spark -> spark: SELECT RLIKE('John Doe', 'John.*')", () => {
    const result = transpile("SELECT RLIKE('John Doe', 'John.*')", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT 'John Doe' RLIKE 'John.*'");
  });
  it("spark -> bigquery: UNHEX(MD5(x))", () => {
    const result = transpile("UNHEX(MD5(x))", { readDialect: DIALECT, writeDialect: "bigquery" })[0];
    expect(result).toBe("FROM_HEX(TO_HEX(MD5(x)))");
  });
  it("spark -> spark: UNHEX(MD5(x))", () => {
    const result = transpile("UNHEX(MD5(x))", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("UNHEX(MD5(x))");
  });
  it.todo("spark -> spark: SELECT * FROM ((VALUES 1)) (unsupported syntax)");
  it("spark -> spark: SELECT CAST(STRUCT('fooo') AS STRUCT<a: VARCHAR(2)>)", () => {
    const result = transpile("SELECT CAST(STRUCT('fooo') AS STRUCT<a: VARCHAR(2)>)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT CAST(STRUCT('fooo' AS col1) AS STRUCT<a: STRING>)");
  });
  it("spark -> : SELECT CAST(123456 AS VARCHAR(3))", () => {
    const result = transpile("SELECT CAST(123456 AS VARCHAR(3))", { readDialect: DIALECT, writeDialect: "" })[0];
    expect(result).toBe("SELECT TRY_CAST(123456 AS TEXT)");
  });
  it("spark -> databricks: SELECT CAST(123456 AS VARCHAR(3))", () => {
    const result = transpile("SELECT CAST(123456 AS VARCHAR(3))", { readDialect: DIALECT, writeDialect: "databricks" })[0];
    expect(result).toBe("SELECT TRY_CAST(123456 AS STRING)");
  });
  it("spark -> spark: SELECT CAST(123456 AS VARCHAR(3))", () => {
    const result = transpile("SELECT CAST(123456 AS VARCHAR(3))", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT CAST(123456 AS STRING)");
  });
  it("spark -> spark2: SELECT CAST(123456 AS VARCHAR(3))", () => {
    const result = transpile("SELECT CAST(123456 AS VARCHAR(3))", { readDialect: DIALECT, writeDialect: "spark2" })[0];
    expect(result).toBe("SELECT CAST(123456 AS STRING)");
  });
  it("spark -> : SELECT TRY_CAST('a' AS INT)", () => {
    const result = transpile("SELECT TRY_CAST('a' AS INT)", { readDialect: DIALECT, writeDialect: "" })[0];
    expect(result).toBe("SELECT TRY_CAST('a' AS INT)");
  });
  it("spark -> databricks: SELECT TRY_CAST('a' AS INT)", () => {
    const result = transpile("SELECT TRY_CAST('a' AS INT)", { readDialect: DIALECT, writeDialect: "databricks" })[0];
    expect(result).toBe("SELECT TRY_CAST('a' AS INT)");
  });
  it("spark -> spark: SELECT TRY_CAST('a' AS INT)", () => {
    const result = transpile("SELECT TRY_CAST('a' AS INT)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT TRY_CAST('a' AS INT)");
  });
  it("spark -> spark2: SELECT TRY_CAST('a' AS INT)", () => {
    const result = transpile("SELECT TRY_CAST('a' AS INT)", { readDialect: DIALECT, writeDialect: "spark2" })[0];
    expect(result).toBe("SELECT CAST('a' AS INT)");
  });
  it.todo("SELECT piv.Q1 FROM (SELECT * FROM produce PIVOT(SUM(sales) FOR quar... (unsupported clause)");
  it.todo("SELECT piv.Q1 FROM (SELECT * FROM (SELECT * FROM produce) PIVOT(SUM... (unsupported clause)");
  it.todo("SELECT * FROM produce PIVOT(SUM(produce.sales) FOR quarter IN ('Q1'... (unsupported clause)");
  it.todo("SELECT * FROM produce AS p PIVOT(SUM(p.sales) AS sales FOR quarter ... (unsupported clause)");
  it.todo("spark -> databricks: SELECT DATEDIFF(MONTH, '2020-01-01', '2020-03-05') (cross-dialect transform)");
  it.todo("spark -> hive: SELECT DATEDIFF(MONTH, '2020-01-01', '2020-03-05') (cross-dialect transform)");
  it.todo("spark -> presto: SELECT DATEDIFF(MONTH, '2020-01-01', '2020-03-05') (unsupported syntax)");
  it("spark -> spark: SELECT DATEDIFF(MONTH, '2020-01-01', '2020-03-05')", () => {
    const result = transpile("SELECT DATEDIFF(MONTH, '2020-01-01', '2020-03-05')", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT DATEDIFF(MONTH, '2020-01-01', '2020-03-05')");
  });
  it.todo("spark -> spark2: SELECT DATEDIFF(MONTH, '2020-01-01', '2020-03-05') (cross-dialect transform)");
  it.todo("spark -> trino: SELECT DATEDIFF(MONTH, '2020-01-01', '2020-03-05') (unsupported syntax)");
  it.todo("SELECT * FROM quarterly_sales PIVOT(SUM(amount) AS amount, 'dummy' ... (unsupported clause)");
  it("spark -> : BOOLEAN(x)", () => {
    const result = transpile("BOOLEAN(x)", { readDialect: DIALECT, writeDialect: "" })[0];
    expect(result).toBe("CAST(x AS BOOLEAN)");
  });
  it("spark -> spark: BOOLEAN(x)", () => {
    const result = transpile("BOOLEAN(x)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("CAST(x AS BOOLEAN)");
  });
  it("spark -> : DATE(x)", () => {
    const result = transpile("DATE(x)", { readDialect: DIALECT, writeDialect: "" })[0];
    expect(result).toBe("CAST(x AS DATE)");
  });
  it("spark -> spark: DATE(x)", () => {
    const result = transpile("DATE(x)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("CAST(x AS DATE)");
  });
  it("spark -> : DOUBLE(x)", () => {
    const result = transpile("DOUBLE(x)", { readDialect: DIALECT, writeDialect: "" })[0];
    expect(result).toBe("CAST(x AS DOUBLE)");
  });
  it("spark -> spark: DOUBLE(x)", () => {
    const result = transpile("DOUBLE(x)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("CAST(x AS DOUBLE)");
  });
  it("spark -> : FLOAT(x)", () => {
    const result = transpile("FLOAT(x)", { readDialect: DIALECT, writeDialect: "" })[0];
    expect(result).toBe("CAST(x AS FLOAT)");
  });
  it("spark -> spark: FLOAT(x)", () => {
    const result = transpile("FLOAT(x)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("CAST(x AS FLOAT)");
  });
  it("spark -> : INT(x)", () => {
    const result = transpile("INT(x)", { readDialect: DIALECT, writeDialect: "" })[0];
    expect(result).toBe("CAST(x AS INT)");
  });
  it("spark -> spark: INT(x)", () => {
    const result = transpile("INT(x)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("CAST(x AS INT)");
  });
  it.todo("spark -> : TIMESTAMP(x) (unsupported syntax)");
  it.todo("spark -> spark: TIMESTAMP(x) (unsupported syntax)");
  it("spark -> : TIMESTAMP_NTZ(x)", () => {
    const result = transpile("TIMESTAMP_NTZ(x)", { readDialect: DIALECT, writeDialect: "" })[0];
    expect(result).toBe("CAST(x AS TIMESTAMPNTZ)");
  });
  it("spark -> spark: TIMESTAMP_NTZ(x)", () => {
    const result = transpile("TIMESTAMP_NTZ(x)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("CAST(x AS TIMESTAMP_NTZ)");
  });
  it.todo("spark -> : TIMESTAMP_LTZ(x) (unsupported syntax)");
  it("spark -> spark: TIMESTAMP_LTZ(x)", () => {
    const result = transpile("TIMESTAMP_LTZ(x)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("CAST(x AS TIMESTAMP_LTZ)");
  });
  it.todo("spark -> : STRING(x) (cross-dialect transform)");
  it("spark -> spark: STRING(x)", () => {
    const result = transpile("STRING(x)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("CAST(x AS STRING)");
  });
  it.todo("CAST(x AS TIMESTAMP) (unsupported syntax)");
  it("spark -> spark: SELECT DATE_ADD(my_date_column, 1)", () => {
    const result = transpile("SELECT DATE_ADD(my_date_column, 1)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT DATE_ADD(my_date_column, 1)");
  });
  it("spark -> spark2: SELECT DATE_ADD(my_date_column, 1)", () => {
    const result = transpile("SELECT DATE_ADD(my_date_column, 1)", { readDialect: DIALECT, writeDialect: "spark2" })[0];
    expect(result).toBe("SELECT DATE_ADD(my_date_column, 1)");
  });
  it.todo("spark -> bigquery: SELECT DATE_ADD(my_date_column, 1) (unsupported syntax)");
  it("spark -> trino: AGGREGATE(my_arr, 0, (acc, x) -> acc + x, s -> s * 2)", () => {
    const result = transpile("AGGREGATE(my_arr, 0, (acc, x) -> acc + x, s -> s * 2)", { readDialect: DIALECT, writeDialect: "trino" })[0];
    expect(result).toBe("REDUCE(my_arr, 0, (acc, x) -> acc + x, s -> s * 2)");
  });
  it("spark -> duckdb: AGGREGATE(my_arr, 0, (acc, x) -> acc + x, s -> s * 2)", () => {
    const result = transpile("AGGREGATE(my_arr, 0, (acc, x) -> acc + x, s -> s * 2)", { readDialect: DIALECT, writeDialect: "duckdb" })[0];
    expect(result).toBe("REDUCE(my_arr, 0, (acc, x) -> acc + x, s -> s * 2)");
  });
  it("spark -> hive: AGGREGATE(my_arr, 0, (acc, x) -> acc + x, s -> s * 2)", () => {
    const result = transpile("AGGREGATE(my_arr, 0, (acc, x) -> acc + x, s -> s * 2)", { readDialect: DIALECT, writeDialect: "hive" })[0];
    expect(result).toBe("REDUCE(my_arr, 0, (acc, x) -> acc + x, s -> s * 2)");
  });
  it("spark -> presto: AGGREGATE(my_arr, 0, (acc, x) -> acc + x, s -> s * 2)", () => {
    const result = transpile("AGGREGATE(my_arr, 0, (acc, x) -> acc + x, s -> s * 2)", { readDialect: DIALECT, writeDialect: "presto" })[0];
    expect(result).toBe("REDUCE(my_arr, 0, (acc, x) -> acc + x, s -> s * 2)");
  });
  it("spark -> spark: AGGREGATE(my_arr, 0, (acc, x) -> acc + x, s -> s * 2)", () => {
    const result = transpile("AGGREGATE(my_arr, 0, (acc, x) -> acc + x, s -> s * 2)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("AGGREGATE(my_arr, 0, (acc, x) -> acc + x, s -> s * 2)");
  });
  it.todo("spark -> spark: TRIM('SL', 'SSparkSQLS') (unsupported syntax)");
  it("spark -> duckdb: ARRAY_SORT(x, (left, right) -> -1)", () => {
    const result = transpile("ARRAY_SORT(x, (left, right) -> -1)", { readDialect: DIALECT, writeDialect: "duckdb" })[0];
    expect(result).toBe("ARRAY_SORT(x)");
  });
  it("spark -> presto: ARRAY_SORT(x, (left, right) -> -1)", () => {
    const result = transpile("ARRAY_SORT(x, (left, right) -> -1)", { readDialect: DIALECT, writeDialect: "presto" })[0];
    expect(result).toBe('ARRAY_SORT(x, ("left", "right") -> -1)');
  });
  it.todo("spark -> hive: ARRAY_SORT(x, (left, right) -> -1) (unsupported syntax)");
  it("spark -> spark: ARRAY_SORT(x, (left, right) -> -1)", () => {
    const result = transpile("ARRAY_SORT(x, (left, right) -> -1)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("ARRAY_SORT(x, (left, right) -> -1)");
  });
  it.todo("ARRAY(0, 1, 2) (unsupported syntax)");
  it.todo("SELECT fname, lname, age FROM person ORDER BY age DESC NULLS FIRST,... (unsupported syntax)");
  it("spark -> duckdb: SELECT APPROX_COUNT_DISTINCT(a) FROM foo", () => {
    const result = transpile("SELECT APPROX_COUNT_DISTINCT(a) FROM foo", { readDialect: DIALECT, writeDialect: "duckdb" })[0];
    expect(result).toBe("SELECT APPROX_COUNT_DISTINCT(a) FROM foo");
  });
  it("spark -> presto: SELECT APPROX_COUNT_DISTINCT(a) FROM foo", () => {
    const result = transpile("SELECT APPROX_COUNT_DISTINCT(a) FROM foo", { readDialect: DIALECT, writeDialect: "presto" })[0];
    expect(result).toBe("SELECT APPROX_DISTINCT(a) FROM foo");
  });
  it("spark -> hive: SELECT APPROX_COUNT_DISTINCT(a) FROM foo", () => {
    const result = transpile("SELECT APPROX_COUNT_DISTINCT(a) FROM foo", { readDialect: DIALECT, writeDialect: "hive" })[0];
    expect(result).toBe("SELECT APPROX_COUNT_DISTINCT(a) FROM foo");
  });
  it("spark -> spark: SELECT APPROX_COUNT_DISTINCT(a) FROM foo", () => {
    const result = transpile("SELECT APPROX_COUNT_DISTINCT(a) FROM foo", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT APPROX_COUNT_DISTINCT(a) FROM foo");
  });
  it("spark -> duckdb: MONTH('2021-03-01')", () => {
    const result = transpile("MONTH('2021-03-01')", { readDialect: DIALECT, writeDialect: "duckdb" })[0];
    expect(result).toBe("MONTH(CAST('2021-03-01' AS DATE))");
  });
  it.todo("spark -> presto: MONTH('2021-03-01') (unsupported syntax)");
  it("spark -> hive: MONTH('2021-03-01')", () => {
    const result = transpile("MONTH('2021-03-01')", { readDialect: DIALECT, writeDialect: "hive" })[0];
    expect(result).toBe("MONTH('2021-03-01')");
  });
  it("spark -> spark: MONTH('2021-03-01')", () => {
    const result = transpile("MONTH('2021-03-01')", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("MONTH('2021-03-01')");
  });
  it("spark -> duckdb: YEAR('2021-03-01')", () => {
    const result = transpile("YEAR('2021-03-01')", { readDialect: DIALECT, writeDialect: "duckdb" })[0];
    expect(result).toBe("YEAR(CAST('2021-03-01' AS DATE))");
  });
  it.todo("spark -> presto: YEAR('2021-03-01') (unsupported syntax)");
  it("spark -> hive: YEAR('2021-03-01')", () => {
    const result = transpile("YEAR('2021-03-01')", { readDialect: DIALECT, writeDialect: "hive" })[0];
    expect(result).toBe("YEAR('2021-03-01')");
  });
  it("spark -> spark: YEAR('2021-03-01')", () => {
    const result = transpile("YEAR('2021-03-01')", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("YEAR('2021-03-01')");
  });
  it("spark -> duckdb: '毛'", () => {
    const result = transpile("'毛'", { readDialect: DIALECT, writeDialect: "duckdb" })[0];
    expect(result).toBe("'毛'");
  });
  it("spark -> presto: '毛'", () => {
    const result = transpile("'毛'", { readDialect: DIALECT, writeDialect: "presto" })[0];
    expect(result).toBe("'毛'");
  });
  it("spark -> hive: '毛'", () => {
    const result = transpile("'毛'", { readDialect: DIALECT, writeDialect: "hive" })[0];
    expect(result).toBe("'毛'");
  });
  it("spark -> spark: '毛'", () => {
    const result = transpile("'毛'", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("'毛'");
  });
  it("spark -> duckdb: SELECT LEFT(x, 2), RIGHT(x, 2)", () => {
    const result = transpile("SELECT LEFT(x, 2), RIGHT(x, 2)", { readDialect: DIALECT, writeDialect: "duckdb" })[0];
    expect(result).toBe("SELECT LEFT(x, 2), RIGHT(x, 2)");
  });
  it("spark -> presto: SELECT LEFT(x, 2), RIGHT(x, 2)", () => {
    const result = transpile("SELECT LEFT(x, 2), RIGHT(x, 2)", { readDialect: DIALECT, writeDialect: "presto" })[0];
    expect(result).toBe("SELECT SUBSTRING(x, 1, 2), SUBSTRING(x, LENGTH(x) - (2 - 1))");
  });
  it("spark -> hive: SELECT LEFT(x, 2), RIGHT(x, 2)", () => {
    const result = transpile("SELECT LEFT(x, 2), RIGHT(x, 2)", { readDialect: DIALECT, writeDialect: "hive" })[0];
    expect(result).toBe("SELECT SUBSTRING(x, 1, 2), SUBSTRING(x, LENGTH(x) - (2 - 1))");
  });
  it("spark -> spark: SELECT LEFT(x, 2), RIGHT(x, 2)", () => {
    const result = transpile("SELECT LEFT(x, 2), RIGHT(x, 2)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT LEFT(x, 2), RIGHT(x, 2)");
  });
  it.todo("SELECT SUBSTR('Spark' FROM 5 FOR 1) (unsupported syntax)");
  it("SELECT SUBSTR('Spark SQL', 5) -> SELECT SUBSTRING('Spark SQL', 5)", () => {
    validateIdentity("SELECT SUBSTR('Spark SQL', 5)", "SELECT SUBSTRING('Spark SQL', 5)");
  });
  it("SELECT SUBSTR(ENCODE('Spark SQL', 'utf-8'), 5) -> SELECT SUBSTRING(ENCODE('Spark SQL', ...", () => {
    validateIdentity("SELECT SUBSTR(ENCODE('Spark SQL', 'utf-8'), 5)", "SELECT SUBSTRING(ENCODE('Spark SQL', 'utf-8'), 5)");
  });
  it.todo("MAP_FROM_ARRAYS(ARRAY(1), c) (unsupported syntax)");
  it("spark -> duckdb: SELECT ARRAY_SORT(x)", () => {
    const result = transpile("SELECT ARRAY_SORT(x)", { readDialect: DIALECT, writeDialect: "duckdb" })[0];
    expect(result).toBe("SELECT ARRAY_SORT(x)");
  });
  it("spark -> presto: SELECT ARRAY_SORT(x)", () => {
    const result = transpile("SELECT ARRAY_SORT(x)", { readDialect: DIALECT, writeDialect: "presto" })[0];
    expect(result).toBe("SELECT ARRAY_SORT(x)");
  });
  it.todo("spark -> hive: SELECT ARRAY_SORT(x) (unsupported syntax)");
  it("spark -> spark: SELECT ARRAY_SORT(x)", () => {
    const result = transpile("SELECT ARRAY_SORT(x)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT ARRAY_SORT(x)");
  });
  it("spark -> spark: SELECT TIMESTAMPADD(MONTH, 20, col)", () => {
    const result = transpile("SELECT TIMESTAMPADD(MONTH, 20, col)", { readDialect: "spark", writeDialect: DIALECT })[0];
    expect(result).toBe("SELECT DATE_ADD(MONTH, 20, col)");
  });
  it("spark -> spark: SELECT DATE_ADD(MONTH, 20, col)", () => {
    const result = transpile("SELECT DATE_ADD(MONTH, 20, col)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT DATE_ADD(MONTH, 20, col)");
  });
  it("spark -> databricks: SELECT DATE_ADD(MONTH, 20, col)", () => {
    const result = transpile("SELECT DATE_ADD(MONTH, 20, col)", { readDialect: DIALECT, writeDialect: "databricks" })[0];
    expect(result).toBe("SELECT DATE_ADD(MONTH, 20, col)");
  });
  it("spark -> presto: SELECT DATE_ADD(MONTH, 20, col)", () => {
    const result = transpile("SELECT DATE_ADD(MONTH, 20, col)", { readDialect: DIALECT, writeDialect: "presto" })[0];
    expect(result).toBe("SELECT DATE_ADD('MONTH', 20, col)");
  });
  it("spark -> trino: SELECT DATE_ADD(MONTH, 20, col)", () => {
    const result = transpile("SELECT DATE_ADD(MONTH, 20, col)", { readDialect: DIALECT, writeDialect: "trino" })[0];
    expect(result).toBe("SELECT DATE_ADD('MONTH', 20, col)");
  });
  it.todo("DESCRIBE schema.test PARTITION(ds = '2024-01-01') (command not supported)");
  it.todo("spark -> duckdb: SELECT ANY_VALUE(col, true), FIRST(col, true), FIRST_VALUE(col, true) ... (cross-dialect transform)");
  it("spark -> spark: SELECT STRUCT(1, 2)", () => {
    const result = transpile("SELECT STRUCT(1, 2)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT STRUCT(1 AS col1, 2 AS col2)");
  });
  it.todo("spark -> presto: SELECT STRUCT(1, 2) (unsupported syntax)");
  it("spark -> duckdb: SELECT STRUCT(1, 2)", () => {
    const result = transpile("SELECT STRUCT(1, 2)", { readDialect: DIALECT, writeDialect: "duckdb" })[0];
    expect(result).toBe("SELECT {'col1': 1, 'col2': 2}");
  });
  it("spark -> spark: SELECT STRUCT(x, 1, y AS col3, STRUCT(5)) FROM t", () => {
    const result = transpile("SELECT STRUCT(x, 1, y AS col3, STRUCT(5)) FROM t", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT STRUCT(x AS x, 1 AS col2, y AS col3, STRUCT(5 AS col1) AS col4) FROM t");
  });
  it("spark -> duckdb: SELECT STRUCT(x, 1, y AS col3, STRUCT(5)) FROM t", () => {
    const result = transpile("SELECT STRUCT(x, 1, y AS col3, STRUCT(5)) FROM t", { readDialect: DIALECT, writeDialect: "duckdb" })[0];
    expect(result).toBe("SELECT {'x': x, 'col2': 1, 'col3': y, 'col4': {'col1': 5}} FROM t");
  });
  it("databricks -> spark: SELECT TIMESTAMPDIFF(MONTH, foo, bar)", () => {
    const result = transpile("SELECT TIMESTAMPDIFF(MONTH, foo, bar)", { readDialect: "databricks", writeDialect: DIALECT })[0];
    expect(result).toBe("SELECT TIMESTAMPDIFF(MONTH, foo, bar)");
  });
  it("spark -> spark: SELECT TIMESTAMPDIFF(MONTH, foo, bar)", () => {
    const result = transpile("SELECT TIMESTAMPDIFF(MONTH, foo, bar)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT TIMESTAMPDIFF(MONTH, foo, bar)");
  });
  it("spark -> databricks: SELECT TIMESTAMPDIFF(MONTH, foo, bar)", () => {
    const result = transpile("SELECT TIMESTAMPDIFF(MONTH, foo, bar)", { readDialect: DIALECT, writeDialect: "databricks" })[0];
    expect(result).toBe("SELECT TIMESTAMPDIFF(MONTH, foo, bar)");
  });
  it.todo("SELECT CAST(col AS TIMESTAMP) (unsupported syntax)");
  it("databricks -> spark: SELECT * FROM {df}", () => {
    const result = transpile("SELECT * FROM {df}", { readDialect: "databricks", writeDialect: DIALECT })[0];
    expect(result).toBe("SELECT * FROM {df}");
  });
  it("spark -> spark: SELECT * FROM {df}", () => {
    const result = transpile("SELECT * FROM {df}", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT * FROM {df}");
  });
  it("spark -> databricks: SELECT * FROM {df}", () => {
    const result = transpile("SELECT * FROM {df}", { readDialect: DIALECT, writeDialect: "databricks" })[0];
    expect(result).toBe("SELECT * FROM {df}");
  });
  it("databricks -> spark: SELECT * FROM {df} WHERE id > :foo", () => {
    const result = transpile("SELECT * FROM {df} WHERE id > :foo", { readDialect: "databricks", writeDialect: DIALECT })[0];
    expect(result).toBe("SELECT * FROM {df} WHERE id > :foo");
  });
  it("spark -> spark: SELECT * FROM {df} WHERE id > :foo", () => {
    const result = transpile("SELECT * FROM {df} WHERE id > :foo", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT * FROM {df} WHERE id > :foo");
  });
  it("spark -> databricks: SELECT * FROM {df} WHERE id > :foo", () => {
    const result = transpile("SELECT * FROM {df} WHERE id > :foo", { readDialect: DIALECT, writeDialect: "databricks" })[0];
    expect(result).toBe("SELECT * FROM {df} WHERE id > :foo");
  });
  it.todo("STRING_AGG(x, ', ') (unsupported syntax)");
  it("spark -> spark, version=3.0.0: LISTAGG(x, ', ')", () => {
    const result = transpile("LISTAGG(x, ', ')", { readDialect: DIALECT, writeDialect: "spark, version=3.0.0" })[0];
    expect(result).toBe("ARRAY_JOIN(COLLECT_LIST(x), ', ')");
  });
  it("spark -> spark, version=4.0.0: LISTAGG(x, ', ')", () => {
    const result = transpile("LISTAGG(x, ', ')", { readDialect: DIALECT, writeDialect: "spark, version=4.0.0" })[0];
    expect(result).toBe("LISTAGG(x, ', ')");
  });
  it("spark -> spark: LISTAGG(x, ', ')", () => {
    const result = transpile("LISTAGG(x, ', ')", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("LISTAGG(x, ', ')");
  });
  it.todo("LIKE(foo, 'pattern') (unsupported syntax)");
  it.todo("LIKE(foo, 'pattern', '!') (unsupported syntax)");
  it.todo("ILIKE(foo, 'pattern') (unsupported syntax)");
  it.todo("ILIKE(foo, 'pattern', '!') (unsupported syntax)");
  it("BITMAP_OR_AGG(x)", () => {
    validateIdentity("BITMAP_OR_AGG(x)");
  });
  it("SELECT ELT(2, 'foo', 'bar', 'baz') AS Result", () => {
    validateIdentity("SELECT ELT(2, 'foo', 'bar', 'baz') AS Result");
  });
  it("SELECT MAKE_INTERVAL(100, 11, 12, 13, 14, 14, 15)", () => {
    validateIdentity("SELECT MAKE_INTERVAL(100, 11, 12, 13, 14, 14, 15)");
  });
  it.todo("SELECT name, GROUPING_ID() FROM customer GROUP BY ROLLUP (name) (unsupported clause)");
  it("SELECT MAKE_TIMESTAMP(2014, 12, 28, 6, 30, 45.887)", () => {
    validateIdentity("SELECT MAKE_TIMESTAMP(2014, 12, 28, 6, 30, 45.887)");
  });
});

describe("Spark: bool_or", () => {
  it("spark -> spark: SELECT a, LOGICAL_OR(b) FROM table GROUP BY a", () => {
    const result = transpile("SELECT a, LOGICAL_OR(b) FROM table GROUP BY a", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT a, BOOL_OR(b) FROM table GROUP BY a");
  });
});

describe("Spark: current_user", () => {
  it("spark -> spark: CURRENT_USER", () => {
    const result = transpile("CURRENT_USER", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("CURRENT_USER()");
  });
  it("spark -> spark: CURRENT_USER()", () => {
    const result = transpile("CURRENT_USER()", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("CURRENT_USER()");
  });
});

describe("Spark: transform_query", () => {
  it("SELECT TRANSFORM(x) USING 'x' AS (x INT) FROM t", () => {
    validateIdentity("SELECT TRANSFORM(x) USING 'x' AS (x INT) FROM t");
  });
  it("SELECT TRANSFORM(zip_code, name, age) USING 'cat' AS (a, b, c) FROM person WHERE zip_co...", () => {
    validateIdentity("SELECT TRANSFORM(zip_code, name, age) USING 'cat' AS (a, b, c) FROM person WHERE zip_code > 94511");
  });
  it("SELECT TRANSFORM(zip_code, name, age) USING 'cat' AS (a STRING, b STRING, c STRING) FRO...", () => {
    validateIdentity("SELECT TRANSFORM(zip_code, name, age) USING 'cat' AS (a STRING, b STRING, c STRING) FROM person WHERE zip_code > 94511");
  });
  it.todo("SELECT TRANSFORM(name, age) ROW FORMAT DELIMITED FIELDS TERMINATED ... (unsupported syntax)");
  it.todo("SELECT TRANSFORM(zip_code, name, age) ROW FORMAT SERDE 'org.apache.... (unsupported syntax)");
  it("SELECT TRANSFORM(zip_code, name, age) USING 'cat' FROM person WHERE zip_code > 94500", () => {
    validateIdentity("SELECT TRANSFORM(zip_code, name, age) USING 'cat' FROM person WHERE zip_code > 94500");
  });
});

describe("Spark: insert_cte", () => {
  it.todo("INSERT OVERWRITE TABLE table WITH cte AS (SELECT cola FROM other_ta... (DDL/DML not supported)");
});

describe("Spark: explode_projection_to_unnest", () => {
  it.todo("spark -> bigquery: SELECT EXPLODE(x) FROM tbl (unsupported syntax)");
  it.todo("spark -> presto: SELECT EXPLODE(x) FROM tbl (unsupported clause)");
  it("spark -> spark: SELECT EXPLODE(x) FROM tbl", () => {
    const result = transpile("SELECT EXPLODE(x) FROM tbl", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT EXPLODE(x) FROM tbl");
  });
  it.todo("spark -> bigquery: SELECT EXPLODE(col) FROM _u (unsupported syntax)");
  it.todo("spark -> presto: SELECT EXPLODE(col) FROM _u (unsupported clause)");
  it("spark -> spark: SELECT EXPLODE(col) FROM _u", () => {
    const result = transpile("SELECT EXPLODE(col) FROM _u", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT EXPLODE(col) FROM _u");
  });
  it.todo("spark -> presto: SELECT EXPLODE(col) AS exploded FROM schema.tbl (unsupported clause)");
  it.todo("SELECT EXPLODE(ARRAY(1, 2)) (unsupported syntax)");
  it.todo("SELECT POSEXPLODE(ARRAY(2, 3)) AS x (unsupported syntax)");
  it.todo("SELECT POSEXPLODE(ARRAY('a')) (unsupported syntax)");
  it.todo("spark -> presto: SELECT POSEXPLODE(x) AS (a, b) (unsupported clause)");
  it.todo("spark -> duckdb: SELECT POSEXPLODE(x) AS (a, b) (cross-dialect transform)");
  it("spark -> spark: SELECT POSEXPLODE(x) AS (a, b)", () => {
    const result = transpile("SELECT POSEXPLODE(x) AS (a, b)", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT POSEXPLODE(x) AS (a, b)");
  });
  it.todo("SELECT * FROM POSEXPLODE(ARRAY('a')) (unsupported syntax)");
  it.todo("SELECT * FROM POSEXPLODE(ARRAY('a')) AS (a, b) (unsupported syntax)");
  it.todo("SELECT POSEXPLODE(ARRAY(2, 3)), EXPLODE(ARRAY(4, 5, 6)) FROM tbl (unsupported syntax)");
  it.todo("SELECT col, pos, POSEXPLODE(ARRAY(2, 3)) FROM _u (unsupported syntax)");
});

describe("Spark: strip_modifiers", () => {
  it.todo("test_strip_modifiers: unresolvable for-loop iterable");
});

describe("Spark: schema_binding_options", () => {
  it.todo("CREATE VIEW emp_v WITH SCHEMA BINDING AS SELECT * FROM emp (DDL/DML not supported)");
  it.todo("CREATE VIEW emp_v WITH SCHEMA COMPENSATION AS SELECT * FROM emp (DDL/DML not supported)");
  it.todo("CREATE VIEW emp_v WITH SCHEMA TYPE EVOLUTION AS SELECT * FROM emp (DDL/DML not supported)");
  it.todo("CREATE VIEW emp_v WITH SCHEMA EVOLUTION AS SELECT * FROM emp (DDL/DML not supported)");
});

describe("Spark: minus", () => {
  it("spark -> spark: SELECT * FROM db.table1 MINUS SELECT * FROM db.table2", () => {
    const result = transpile("SELECT * FROM db.table1 MINUS SELECT * FROM db.table2", { readDialect: DIALECT, writeDialect: "spark" })[0];
    expect(result).toBe("SELECT * FROM db.table1 EXCEPT SELECT * FROM db.table2");
  });
  it("spark -> databricks: SELECT * FROM db.table1 MINUS SELECT * FROM db.table2", () => {
    const result = transpile("SELECT * FROM db.table1 MINUS SELECT * FROM db.table2", { readDialect: DIALECT, writeDialect: "databricks" })[0];
    expect(result).toBe("SELECT * FROM db.table1 EXCEPT SELECT * FROM db.table2");
  });
});

describe("Spark: string", () => {
  it.todo("test_string: assertEqual call");
  it.todo("test_string: assertEqual call (2)");
  it.todo("test_string: assertEqual call (3)");
  it.todo("test_string: assertEqual call (4)");
});

describe("Spark: binary_string", () => {
  it.todo("test_binary_string: assertEqual call");
  it.todo("test_binary_string: assertEqual call (2)");
  it.todo("test_binary_string: assertEqual call (3)");
  it.todo("test_binary_string: assertEqual call (4)");
  it.todo("test_binary_string: assertEqual call (5)");
  it.todo("test_binary_string: assertEqual call (6)");
});

describe("Spark: analyze", () => {
  it.todo("ANALYZE TABLE tbl COMPUTE STATISTICS NOSCAN (command not supported)");
  it.todo("ANALYZE TABLE tbl COMPUTE STATISTICS FOR ALL COLUMNS (command not supported)");
  it.todo("ANALYZE TABLE tbl COMPUTE STATISTICS FOR COLUMNS foo, bar (command not supported)");
  it.todo("ANALYZE TABLE ctlg.db.tbl COMPUTE STATISTICS NOSCAN (command not supported)");
  it.todo("ANALYZE TABLES COMPUTE STATISTICS NOSCAN (command not supported)");
  it.todo("ANALYZE TABLES FROM db COMPUTE STATISTICS (command not supported)");
  it.todo("ANALYZE TABLES IN db COMPUTE STATISTICS (command not supported)");
  it.todo("ANALYZE TABLE ctlg.db.tbl PARTITION(foo = 'foo', bar = 'bar') COMPU... (command not supported)");
});

describe("Spark: transpile_annotated_exploded_column", () => {
  it.todo("test_transpile_annotated_exploded_column: assertEqual call");
  it.todo("test_transpile_annotated_exploded_column: assertEqual call (2)");
  it.todo("test_transpile_annotated_exploded_column: assertEqual call (3)");
  it.todo("test_transpile_annotated_exploded_column: assertEqual call (4)");
});

describe("Spark: approx_percentile", () => {
  it("spark -> spark: APPROX_PERCENTILE(DISTINCT col, 0.3)", () => {
    const result = transpile("APPROX_PERCENTILE(DISTINCT col, 0.3)", { readDialect: "spark", writeDialect: DIALECT })[0];
    expect(result).toBe("PERCENTILE_APPROX(DISTINCT col, 0.3)");
  });
  it("databricks -> spark: APPROX_PERCENTILE(DISTINCT col, 0.3)", () => {
    const result = transpile("APPROX_PERCENTILE(DISTINCT col, 0.3)", { readDialect: "databricks", writeDialect: DIALECT })[0];
    expect(result).toBe("PERCENTILE_APPROX(DISTINCT col, 0.3)");
  });
  it("spark -> spark: APPROX_PERCENTILE(DISTINCT col, 0.3, 200)", () => {
    const result = transpile("APPROX_PERCENTILE(DISTINCT col, 0.3, 200)", { readDialect: "spark", writeDialect: DIALECT })[0];
    expect(result).toBe("PERCENTILE_APPROX(DISTINCT col, 0.3, 200)");
  });
  it("databricks -> spark: APPROX_PERCENTILE(DISTINCT col, 0.3, 200)", () => {
    const result = transpile("APPROX_PERCENTILE(DISTINCT col, 0.3, 200)", { readDialect: "databricks", writeDialect: DIALECT })[0];
    expect(result).toBe("PERCENTILE_APPROX(DISTINCT col, 0.3, 200)");
  });
});

describe("Spark: array_insert", () => {
  it.todo("SELECT ARRAY_INSERT(ARRAY('a', 'b', 'c'), 1, 'z') (unsupported syntax)");
});
